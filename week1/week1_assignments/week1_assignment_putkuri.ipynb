{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "In this week's assignments, you will train a LightGBM regression model to predict public bike sharing demand given the attributes like datetime and weather conditions. The dataset (\"bike_sharing_demand.csv\", located under the same directory as this notebook) used in this week's assignments is a preprocessed version of [this kaggle dataset](https://www.kaggle.com/competitions/bike-sharing-demand/overview). You'll learn more about data preprocessing next week. Additionally, you'll use MLflow to track the model training and Deepchecks to evaluate the trained model. \n",
    "\n",
    "**Guidelines of submitting assignments**:\n",
    "- For each assignment, a code skeleton is provided. Please put your solutions in between the `### START CODE HERE` and `### END CODE HERE` code comments. \n",
    "- Some assignments also require you to answer questions or capture screenshots in order to earn points. Please put all your answers and the required screenshots into a single PDF file. For each answer or screenshot, please clearly indicate which assignment it corresponds to in your PDF file. \n",
    "- When preparing your submission, be sure to include your assignment notebook with code cell outputs. It's important that these outputs are current and reflect the latest state of your code, as your grades may depend on them. Additionally, please include the PDF file that contains your answers and screenshots in your submission. \n",
    "\n",
    "In case type hints are new to you, you'll see something below in some code skeletons:\n",
    "```python\n",
    "def greeting(name: str) -> str:\n",
    "    return 'Hello ' + name  \n",
    "```\n",
    "The annotation `name: str` means the parameter `name` is expected to be of type `str` and `-> str` means the type of the returned value is also `str`. These type hints are provided to help you understand the function's input requirements and its expected output in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 0: Set up the course environment (4 points)\n",
    "You can earn 4 points for successfully setting up the course environment. To do so, simply write \"yes\" in your PDF file. Additionally, feel free to provide suggestions for improving the instructions within your PDF file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes I did the setup\n",
    "would be beneficial to have instructions on Mac also, the recommended VM did not work on the M1 or M2 systems "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "import mlflow\n",
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular.checks import TrainTestPerformance, ModelInferenceTime, MultiModelPerformanceReport\n",
    "from deepchecks.tabular import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Download and split the dataset (2 points)\n",
    "\n",
    "### 1a) Load the data\n",
    "First, implement the `pull_data` function that loads a CSV as a Pandas DataFrame from a given location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_data(dataset_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download the data set from a given path\n",
    "    Args: \n",
    "        dataset_path (Path): Path of the CSV \n",
    "    Returns:\n",
    "        A Pandas DataFrame of the dataset\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(dataset_path) # this reads the csv file into a pandas dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      10886 non-null  int64  \n",
      " 1   holiday     10886 non-null  int64  \n",
      " 2   workingday  10886 non-null  int64  \n",
      " 3   weather     10886 non-null  int64  \n",
      " 4   temp        10886 non-null  float64\n",
      " 5   atemp       10886 non-null  float64\n",
      " 6   humidity    10886 non-null  int64  \n",
      " 7   windspeed   10886 non-null  float64\n",
      " 8   count       10886 non-null  int64  \n",
      " 9   hour        10886 non-null  int64  \n",
      " 10  day         10886 non-null  int64  \n",
      " 11  month       10886 non-null  int64  \n",
      "dtypes: float64(3), int64(9)\n",
      "memory usage: 1020.7 KB\n"
     ]
    }
   ],
   "source": [
    "# You can use this code cell to check if your pull_data function works correctly\n",
    "dataset_path = Path.cwd() / \"bike_sharing_demand.csv\"\n",
    "df = pull_data(dataset_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Expected output</summary>\n",
    "    <img src=\"./images/dataset-info.png\"/>\n",
    "</details>\n",
    "\n",
    "Below is the explanation of each column in the dataset:\n",
    "\n",
    "**Variables**:\n",
    "\n",
    "| Column name |  Explanation | type |\n",
    "|-------------|---------------|----|\n",
    "| season      | 1 = spring, 2 = summer, 3 = fall, 4 = winter | integer\n",
    "| holiday     | whether the day is considered a holiday | integer\n",
    "| workingday  | 1 if day is neither weekend nor holiday, otherwise 0. | integer\n",
    "| weather     | 1: Clear, Few clouds, Partly cloudy, Partly cloudy; 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist; 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds; 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog | integer\n",
    "| temp        | temperature in Celsius | float\n",
    "| atemp       | \"feels like\" temperature in Celsius | float\n",
    "| humidity    | relative humidity | integer\n",
    "| windspeed   | wind speed | float\n",
    "| hour        | the hours of the datetime| integer\n",
    "| day         | the day of the datetime| integer\n",
    "| month       | the month of the datetime| integer\n",
    "\n",
    "**Targets**: \n",
    "\n",
    "| Column name | Explanation                                     | Type\n",
    "|-------------|-------------------------------------------------| ----|\n",
    "| count       | number of total rentals                         | integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b) Split the data into train and test DataFrames\n",
    "Then implement the `splitData` function that splits the dataset into a training and a test dataset, using the last 168 rows of the dataset as the test data.\n",
    "\n",
    "Hint: You may find [pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/version/1.5/reference/api/pandas.DataFrame.iloc.html) useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(input_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Split a DataFrame into training and testing sets using the last 168 rows as the test set\n",
    "    Args:\n",
    "        input_df (DataFrame): The DataFrame to be splitted\n",
    "    Return:\n",
    "        A tuple of training and testing DataFrame\n",
    "    \"\"\"\n",
    "    train_df = input_df.iloc[:-168]\n",
    "    test_df = input_df.iloc[-168:]\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "      <td>10718.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.483206</td>\n",
       "      <td>0.029017</td>\n",
       "      <td>0.680351</td>\n",
       "      <td>1.418268</td>\n",
       "      <td>20.327554</td>\n",
       "      <td>23.751554</td>\n",
       "      <td>61.777944</td>\n",
       "      <td>12.847561</td>\n",
       "      <td>191.275518</td>\n",
       "      <td>11.542265</td>\n",
       "      <td>9.898395</td>\n",
       "      <td>6.435622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.108993</td>\n",
       "      <td>0.167861</td>\n",
       "      <td>0.466362</td>\n",
       "      <td>0.634893</td>\n",
       "      <td>7.806103</td>\n",
       "      <td>8.496143</td>\n",
       "      <td>19.250565</td>\n",
       "      <td>8.174376</td>\n",
       "      <td>181.304517</td>\n",
       "      <td>6.915736</td>\n",
       "      <td>5.461328</td>\n",
       "      <td>3.401736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.940000</td>\n",
       "      <td>16.665000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.001500</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>24.240000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>12.998000</td>\n",
       "      <td>144.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.240000</td>\n",
       "      <td>31.060000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>16.997900</td>\n",
       "      <td>283.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>45.455000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>56.996900</td>\n",
       "      <td>977.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             season       holiday    workingday       weather          temp  \\\n",
       "count  10718.000000  10718.000000  10718.000000  10718.000000  10718.000000   \n",
       "mean       2.483206      0.029017      0.680351      1.418268     20.327554   \n",
       "std        1.108993      0.167861      0.466362      0.634893      7.806103   \n",
       "min        1.000000      0.000000      0.000000      1.000000      0.820000   \n",
       "25%        1.000000      0.000000      0.000000      1.000000     13.940000   \n",
       "50%        2.000000      0.000000      1.000000      1.000000     20.500000   \n",
       "75%        3.000000      0.000000      1.000000      2.000000     26.240000   \n",
       "max        4.000000      1.000000      1.000000      4.000000     41.000000   \n",
       "\n",
       "              atemp      humidity     windspeed         count          hour  \\\n",
       "count  10718.000000  10718.000000  10718.000000  10718.000000  10718.000000   \n",
       "mean      23.751554     61.777944     12.847561    191.275518     11.542265   \n",
       "std        8.496143     19.250565      8.174376    181.304517      6.915736   \n",
       "min        0.760000      0.000000      0.000000      1.000000      0.000000   \n",
       "25%       16.665000     47.000000      7.001500     42.000000      6.000000   \n",
       "50%       24.240000     62.000000     12.998000    144.500000     12.000000   \n",
       "75%       31.060000     77.000000     16.997900    283.000000     18.000000   \n",
       "max       45.455000    100.000000     56.996900    977.000000     23.000000   \n",
       "\n",
       "                day         month  \n",
       "count  10718.000000  10718.000000  \n",
       "mean       9.898395      6.435622  \n",
       "std        5.461328      3.401736  \n",
       "min        1.000000      1.000000  \n",
       "25%        5.000000      3.000000  \n",
       "50%       10.000000      6.000000  \n",
       "75%       15.000000      9.000000  \n",
       "max       19.000000     12.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pull_data(dataset_path)\n",
    "train, test = splitData(df)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Expected output</summary>\n",
    "    <img src=\"./images/train-df.png\"/>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of the training dataset should be (10718, 12)\n",
      "The dimension of the testing dataset should be (168, 12)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dimension of the training dataset should be {train.shape}\")\n",
    "print(f\"The dimension of the testing dataset should be {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "The dimension of the training dataset should be (10718, 12)\n",
    "\n",
    "The dimension of the testing dataset should be (168, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing DataFrames into features and targets\n",
    "target = \"count\"\n",
    "input_df = pull_data(Path.cwd() / \"bike_sharing_demand.csv\")\n",
    "train, test = splitData(input_df)\n",
    "train_x = train.drop([target], axis=1)\n",
    "test_x = test.drop([target], axis=1)\n",
    "train_y = train[[target]]\n",
    "test_y = test[[target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Offline model evaluation using Deepchecks (2 points)\n",
    "\n",
    "### 2a) Construct the Dataset objects used by Deepchecks\n",
    "First, let's construct the Deepchecks Dataset objects from the train and testing DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features, remember to specify the categorical features when you construct \n",
    "# the datasets handled by Deepchecks\n",
    "# See https://docs.deepchecks.com/stable/tabular/usage_guides/dataset_object.html\n",
    "cat_features = [\"season\", \"holiday\", \"workingday\", \"weather\", \"hour\", \"day\", \"month\"]\n",
    "\n",
    "### START CODE HERE\n",
    "train_dataset = Dataset(df = train_x, label = train_y, cat_features = cat_features)\n",
    "test_dataset = Dataset(df = test_x, label = test_y, cat_features = cat_features)\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b) Deepchecks Suite with conditions\n",
    "Implement the `evaluate` function that uses Deepchecks Suite to perform the following two tests:\n",
    "1) Evaluate the model's MAE and RMSE on both training and testing dataset. This test should fail if the MAE or RMSE drops more than 20% on the testing dataset compared to the training dataset;\n",
    "2) Evaluate the model's inference time. This test should fail if the inference time exceeds 0.1 second. \n",
    "\n",
    "Additionally, this function should save the evaluation results into an HTML file under the same directory as this notebook. \n",
    "\n",
    "**Hints**:\n",
    "- [How to add conditions to a test?](https://docs.deepchecks.com/stable/general/usage/customizations/auto_examples/plot_configure_check_conditions.html)\n",
    "- [Train test performance](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.html)\n",
    "- [Model inference time](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_model_inference_time.html)\n",
    "- [Condition for comparing model performance between training and testing dataset](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than.html#deepchecks.tabular.checks.model_evaluation.TrainTestPerformance.add_condition_train_test_relative_degradation_less_than)\n",
    "- [Condition for validating inference time](https://docs.deepchecks.com/stable/api/generated/deepchecks.tabular.checks.model_evaluation.ModelInferenceTime.add_condition_inference_time_less_than.html)\n",
    "- [How to export the evaluation result into a file?](https://docs.deepchecks.com/stable/general/usage/export_save_results.html#save-result-as-an-html-report-save-as-html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_dataset: Dataset, test_dataset: Dataset, model: LGBMRegressor, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Use Deepchecks to evaluate:\n",
    "    1) model's MAE and RMSE on both training and testing dataset,\n",
    "    2) model's inference time. Then save the evaluation results to an HTML file\n",
    "    Args:\n",
    "        train_dataset (Dataset): training Dataset\n",
    "        test_dataset (Dataset): testing Dataset\n",
    "        model (LGBMRegressor): The LightGBM regression model to be evaluated\n",
    "        filename: The file name of the evaluation results\n",
    "    Return:\n",
    "        Name of the created HTML file\n",
    "    \"\"\"\n",
    "    suite = Suite(\"Bike test suite\", \n",
    "    TrainTestPerformance(scorers=[\"neg_mae\", \"neg_rmse\"]).add_condition_train_test_relative_degradation_less_than(0.2),\n",
    "    ModelInferenceTime().add_condition_inference_time_less_than(value=0.1)\n",
    "    )\n",
    "    lr = model\n",
    "    result = suite.run(model=lr,\n",
    "                        train_dataset=train_dataset,\n",
    "                        test_dataset=test_dataset)\n",
    "\n",
    "    #result.show()\n",
    "        \n",
    "    result.save_as_html(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a testing model trained on the same bike demand dataset to help you check if your evaluate function works correctly\n",
    "test_model = pickle.load(open(\"test-model.pkl\", \"rb\"))\n",
    "result_file_name = evaluate(train_dataset, test_dataset, test_model, \"test-result.html\")\n",
    "print(result_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the above code cell, you should see a file named \"test-result.html\" appear under the same directory with this notebook.\n",
    "\n",
    "Let's render this HTML file in a browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "webbrowser.open(\"file://\" + str(Path.cwd() / result_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> Expected output (in the browser) </summary>\n",
    "    <br />\n",
    "    The test of MAE/RMSE should fail:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-train-test-performance.png\">\n",
    "    <br />\n",
    "    The test of inference time should pass:\n",
    "    <br />\n",
    "    <img src=\"./images/deepchecks-inference-time.png\">\n",
    "</details>\n",
    "\n",
    "### Screenshots to be submitted for Assignment 2\n",
    "Similar to the expected output above, please submit screenshots of the web page showing which test passed and which test failed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3: Tracking model training in MLflow (2 points)\n",
    "Similar to what you see in the MLflow tutorial, you need to complete the following tasks in this assignment: \n",
    "1) Use LightGBM to train a regression model to predict the bike sharing demand. The model should be trained using the training DataFrame you prepared previously and with the following hyperparameters: num_leaves = 63, learning_rate = 0.05, random_state = 42 (for reproducibility).\n",
    "2) Use MLflow to track the model training. Specifically, you need to log the used hyperparameters and use the `evaluation` function you created above to evaluate the trained model. You also need to upload the Deepchecks evaluation result file and register the trained model to MLflow.  \n",
    "\n",
    "**Hint**: You can check [LightGBM documentation on how to train a regression model](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#) if you're not familiar with it.\n",
    "\n",
    "**Note**: LightGBM provides two types of APIs, the native APIs and the Scikit-learn (sklearn) APIs. The sklearn APIs can be seen as an interface that allows you to use a LightGBM model as if it was trained using the sklearn library. The model trained in this assignment will be used in other assignments in the upcoming weeks. Please use the **sklearn API** in this assignment to avoid unexpected issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow configuration\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_S3_ENDPOINT_URL = os.getenv(\"MLFLOW_S3_ENDPOINT_URL\")\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "mlflow_experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\")\n",
    "\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = MLFLOW_S3_ENDPOINT_URL\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n",
    "\n",
    "# model hyperparameters\n",
    "num_leaves = 63\n",
    "learning_rate = 0.05\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fitting model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 289\n",
      "[LightGBM] [Info] Number of data points in the train set: 10718, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 191.275518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Model fitted.\n",
      "INFO:__main__:Predicting on test set...\n",
      "INFO:__main__:Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:LightGBMRegressor model (num_leaves=63.000000, learning_rate=0.050000, random_state=42.000000):\n",
      "INFO:__main__:Logging parameters to MLflow\n",
      "INFO:__main__:Logging Deepchecks evaluation results to MLflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run id: 8c3f1bf81d8e4a8a9eeb66247944476d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Logging trained model\n",
      "Registered model 'LightGBMRegressor' already exists. Creating a new version of this model...\n",
      "2023/11/09 18:13:14 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: LightGBMRegressor, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The S3 URI of the logged model: s3://mlflow/5/8c3f1bf81d8e4a8a9eeb66247944476d/artifacts/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '7' of model 'LightGBMRegressor'.\n"
     ]
    }
   ],
   "source": [
    "model = LGBMRegressor(num_leaves=num_leaves, learning_rate=learning_rate, random_state=random_state)\n",
    "import logging\n",
    "id = \"Bike_example\"\n",
    "with mlflow.start_run() as run:\n",
    "    ### START CODE HERE\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    lr = model\n",
    "    logger.info(\"Fitting model...\")\n",
    "    lr.fit(train_x, train_y)\n",
    "    logger.info(\"Model fitted.\")\n",
    "\n",
    "    logger.info(\"Predicting on test set...\")\n",
    "    preds = lr.predict(test_x)\n",
    "\n",
    "    \n",
    "    logger.info(\"Evaluating model...\")\n",
    "    evaluate(train_dataset, test_dataset, lr, \"result.html\")\n",
    "    logger.info(\"LightGBMRegressor model (num_leaves=%f, learning_rate=%f, random_state=%f):\" % (num_leaves, learning_rate, random_state))\n",
    "\n",
    "\n",
    "    logger.info(\"Logging parameters to MLflow\")\n",
    "    mlflow.log_param(\"num_leaves\", num_leaves)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    \n",
    "    # print the mlflow run id \n",
    "    print(\"MLflow run id: %s\" % run.info.run_id)\n",
    "\n",
    "    logger.info(\"Logging Deepchecks evaluation results to MLflow\")\n",
    "    result_file_name = evaluate(train_dataset, test_dataset, lr, \"lightGBMRegressor.html\")\n",
    "    mlflow.log_artifact(\"lightGBMRegressor.html\")\n",
    "\n",
    "    logger.info(\"Logging trained model\")\n",
    "    artifact_name = \"model\"\n",
    "    mlflow.sklearn.log_model(\n",
    "        lr, artifact_name, registered_model_name=\"LightGBMRegressor\")\n",
    "    print(\"The S3 URI of the logged model:\", mlflow.get_artifact_uri(artifact_path=artifact_name))\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots to be submitted for Assignment 3\n",
    "To get the points from Assignment 3, please submit the following screenshots:\n",
    "1. The logs of your MLflow run. Please include the parameters of the model in your screenshot. \n",
    "\n",
    "<img src=\"./images/Week1-E3.1.png\" width=1000>\n",
    "\n",
    "\n",
    "2. The details of the MLflow run including uploaded Deepchecks evaluation result file;\n",
    "\n",
    "<img src=\"./images/Week1-E3.2.png\" width=1000>\n",
    "<img src=\"./images/Week1-E3.2.2.png\" width=1000>\n",
    "<img src=\"./images/Week1-E3.2.3.png\" width=1000>\n",
    "\n",
    "\n",
    "3. The registered model.\n",
    "\n",
    "<img src=\"./images/Week1-E3.1.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4: Evaluate the trained model against another model (2 points)\n",
    "Suppose your colleague had trained an ElasticNet model for the same use case of bike sharing demand prediction. In this assignment, your tasks are:\n",
    "1. Using Deepchecks to compare MAE and RMSE of your LightGBM model to the ElasticNet model and saving the result as an HTML file;\n",
    "1. Uploading the result file to MLflow, the file should be under the MLflow Run where you trained your LightGBM model in Assignment 3. E.g.,\n",
    "\n",
    "<img src=\"./images/ass4-example.png\" width=300 />\n",
    "\n",
    "Note that the idea here is to attach a file to an existing MLflow Run, not to create a new MLflow Run and then upload the file under the new MLflow Run. \n",
    "\n",
    "You may find the following docs helpful: \n",
    "- [Multi model performance report](https://docs.deepchecks.com/stable/tabular/auto_checks/model_evaluation/plot_multi_model_performance_report.html).\n",
    "- [mlflow.start_run](https://mlflow.org/docs/2.3.2/python_api/mlflow.html#mlflow.start_run) (Pay attention to the use of the `run_id` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 289\n",
      "[LightGBM] [Info] Number of data points in the train set: 10718, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score 191.275518\n"
     ]
    }
   ],
   "source": [
    "# Load the ElasticNet model first\n",
    "old_model = pickle.load(open(\"old-model.pkl\", \"rb\"))\n",
    "model = LGBMRegressor(num_leaves=num_leaves, learning_rate=learning_rate, random_state=random_state).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comparison (1).html'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Deepchecks to compare your LightGBM model to the ElasticNet model and save the result to an HTML file\n",
    "### START CODE HERE\n",
    "#train_dataset = Dataset(df = train_x, label = train_y, cat_features = cat_features)\n",
    "#test_dataset = Dataset(df = test_x, label = test_y, cat_features = cat_features)\n",
    "\n",
    "\n",
    "clf1 = old_model\n",
    "clf2 = model\n",
    "\n",
    "\n",
    "result = MultiModelPerformanceReport().run(train_dataset, test_dataset, [clf2, clf1])\n",
    "\n",
    "# save the result to an HTML file\n",
    "result.save_as_html(\"comparison.html\")\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run:  cd398a8a39794e37a73118852acbee36\n"
     ]
    }
   ],
   "source": [
    "# Upload the result file to MLflow. The file should be under the MLflow Run where you trained your LightGBM model\n",
    "### START CODE HERE\n",
    "with mlflow.start_run(run_id=\"cd398a8a39794e37a73118852acbee36\") as run:\n",
    "    ### START CODE HERE\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # print the start of the MLflow run\n",
    "    print(\"MLflow run: \", run.info.run_id)\n",
    "    # add the comparison.html file to the MLflow run\n",
    "    mlflow.log_artifact(\"comparison.html\")\n",
    "    \n",
    "    \n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screenshots to be submitted for Assignment 4\n",
    "The details of the MLflow run including uploaded Deepchecks model comparison result file.\n",
    "\n",
    "Result file:\n",
    "\n",
    "<img src=\"./images/Week1-E4.png\" >\n",
    "<details>\n",
    "    <summary>Example</summary>\n",
    "    <img src=\"./images/deepchecks-compare-models.png\" />\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
